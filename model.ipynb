{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:36:58.535393Z","iopub.status.busy":"2024-03-16T15:36:58.534978Z","iopub.status.idle":"2024-03-16T15:37:01.991666Z","shell.execute_reply":"2024-03-16T15:37:01.990584Z","shell.execute_reply.started":"2024-03-16T15:36:58.535349Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import random\n","import torch\n","import os\n","\n","def seed_everything(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","seed_everything(21)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:01.993582Z","iopub.status.busy":"2024-03-16T15:37:01.993064Z","iopub.status.idle":"2024-03-16T15:37:16.794863Z","shell.execute_reply":"2024-03-16T15:37:16.793576Z","shell.execute_reply.started":"2024-03-16T15:37:01.993535Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n","Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n","Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:16.797235Z","iopub.status.busy":"2024-03-16T15:37:16.796903Z","iopub.status.idle":"2024-03-16T15:37:20.348402Z","shell.execute_reply":"2024-03-16T15:37:20.347389Z","shell.execute_reply.started":"2024-03-16T15:37:16.797205Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6000/6000 [00:01<00:00, 4416.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Completed pre-processing train texts...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1801/1801 [00:00<00:00, 4243.99it/s]"]},{"name":"stdout","output_type":"stream","text":["Completed pre-processing test texts...\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# preprocessing text\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stopw = stopwords.words('english')\n","from nltk.stem import PorterStemmer\n","\n","from tqdm import tqdm\n","import unicodedata\n","import contractions\n","from bs4 import BeautifulSoup\n","\n","def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    [s.extract() for s in soup(['iframe', 'script'])]\n","    stripped_text = soup.get_text()\n","    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n","    return stripped_text\n","\n","def remove_accented_chars(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","def pre_process_corpus(docs):\n","    norm_docs = []\n","    for doc in tqdm(docs):\n","        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n","        doc = doc.lower()\n","        doc = remove_accented_chars(doc)\n","        doc = contractions.fix(doc)\n","        # remove special characters\\whitespaces\n","        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n","        doc = re.sub(' +', ' ', doc)\n","        # remove stop words\n","        #doc = ' '.join(word for word in str(doc).split() if word not in stopw)  \n","        doc = doc.strip()\n","        #stemmer = PorterStemmer()\n","        #doc = [stemmer.stem(word) for word in doc.split()]\n","        # Join the stemmed words back into a single string\n","        #doc = ' '.join(doc)\n","        norm_docs.append(doc)\n","\n","    return norm_docs\n","\n","train_data['cleaned_text'] = pre_process_corpus(train_data['text'])\n","print(\"Completed pre-processing train texts...\")\n","\n","test_data['cleaned_text'] = pre_process_corpus(test_data['text'])\n","print(\"Completed pre-processing test texts...\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:20.352352Z","iopub.status.busy":"2024-03-16T15:37:20.351675Z","iopub.status.idle":"2024-03-16T15:37:25.284987Z","shell.execute_reply":"2024-03-16T15:37:25.284161Z","shell.execute_reply.started":"2024-03-16T15:37:20.352321Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb2479d6f8354aa99a87c5dd35eae67b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"143ade65619043138f972a150709abe6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fb7798dc04f4e069dcdd67c94e3fddd","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18683737e0d347ff80172d4e8cc7cb60","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# https://huggingface.co/docs/transformers/model_doc/roberta \n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoTokenizer, DistilBertTokenizer\n","from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# define tokenizer\n","\n","#tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_length=512)\n","\n","#model_name = \"nghuyong/ernie-2.0-base-en\"\n","#tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","#tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:25.286510Z","iopub.status.busy":"2024-03-16T15:37:25.286056Z","iopub.status.idle":"2024-03-16T15:37:38.482461Z","shell.execute_reply":"2024-03-16T15:37:38.481181Z","shell.execute_reply.started":"2024-03-16T15:37:25.286484Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:38.484618Z","iopub.status.busy":"2024-03-16T15:37:38.484196Z","iopub.status.idle":"2024-03-16T15:37:39.069625Z","shell.execute_reply":"2024-03-16T15:37:39.068576Z","shell.execute_reply.started":"2024-03-16T15:37:38.484574Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Dict:\n"," DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'stars', 'cleaned_text'],\n","        num_rows: 6000\n","    })\n","    eval: Dataset({\n","        features: ['ID', 'text', 'cleaned_text'],\n","        num_rows: 1801\n","    })\n","})\n","\n","\n","Train's features:\n"," {'text': Value(dtype='string', id=None), 'stars': Value(dtype='int64', id=None), 'cleaned_text': Value(dtype='string', id=None)}\n","\n","\n","First row of Train:\n"," will never come back horrible service nasty boba drink i used to live in california and had always gone to tea station the tea station in vegas is just horrible my boyfriend and i went to tea station last week we had a peppermint milk tea w coffee jelly and a wheat germ milk tea w boba the bill is 1405 seriously 2 drinks for 14and the drink is not even good the waitress has some attitude problems and she messed up our order our waitress is so rude and the drink is so watery she literally just threw our drinks on the table and she was so impatient while taking our orders i mean i have been to many asian restauranti know how most restaurants services are and i can tolerate it because of the food but tea station heck no bad food and horrible service there are many boba place is vegas like easy life no 1 beegee i do not know why on earth i ended up here there is no way that im going to go back to that place for crappy drinks and horrible services i do not know why they are still in business\n"]}],"source":["from datasets import Dataset, DatasetDict\n","\n","train_data['stars'] = train_data['stars'] - 1\n","\n","raw_datasets = DatasetDict({\n","    \"train\": Dataset.from_pandas(train_data),\n","    \"eval\": Dataset.from_pandas(test_data)\n","})\n","\n","# Check the datasets\n","print(\"Dataset Dict:\\n\", raw_datasets)\n","print(\"\\n\\nTrain's features:\\n\", raw_datasets[\"train\"].features)\n","print(\"\\n\\nFirst row of Train:\\n\", raw_datasets[\"train\"]['cleaned_text'][0])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:37:39.071390Z","iopub.status.busy":"2024-03-16T15:37:39.070890Z","iopub.status.idle":"2024-03-16T15:38:29.457541Z","shell.execute_reply":"2024-03-16T15:38:29.456601Z","shell.execute_reply.started":"2024-03-16T15:37:39.071363Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f36372ccd4d4c81aa8dac5f54f3bc3c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a51c0159f6a143f583937967edb98de6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'stars', 'cleaned_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 6000\n","    })\n","    eval: Dataset({\n","        features: ['ID', 'text', 'cleaned_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1801\n","    })\n","})\n"]}],"source":["# Tokenize the text, and truncate the text if it exceed the tokenizer maximum length. Batched=True to tokenize multiple texts at the same time.\n","tokenized_datasets = raw_datasets.map(lambda dataset: tokenizer(dataset['cleaned_text'], truncation=True), batched=True)\n","\n","print(tokenized_datasets)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:29.459623Z","iopub.status.busy":"2024-03-16T15:38:29.459222Z","iopub.status.idle":"2024-03-16T15:38:29.477854Z","shell.execute_reply":"2024-03-16T15:38:29.476925Z","shell.execute_reply.started":"2024-03-16T15:38:29.459586Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["so right away if i go into a buffet setting and there are signs indicating for you to eat everything that you take right away to me that means they are having issues with food costs and their business is not doing too wellnnsome things on a buffet i do not expect much quality sushi on a buffet i expect to be mediocre dumplings i expect to be frozen ones that are steameddeep fried here the sushi was atrocious i do not think i have ever had worse sushi anywhere the dumplings were ok but it is hard to screw up reheating dumplingsnnthe hibachimongolian bbq service was good and would be the only reason for me to come back as that is something i missed from a place i used to go to often in wausaunnhowever if i cannot finish what is on my plate i do not think a restaurant should be upset with me for it cut down how much crap you have out there that no one touches or is just plain bad especially the abysmal sushi and watch your profits pick back up\n"]}],"source":["# Check the first row\n","print(tokenized_datasets[\"train\"]['cleaned_text'][2])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:29.479722Z","iopub.status.busy":"2024-03-16T15:38:29.479296Z","iopub.status.idle":"2024-03-16T15:38:29.493260Z","shell.execute_reply":"2024-03-16T15:38:29.492172Z","shell.execute_reply.started":"2024-03-16T15:38:29.479654Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'labels', 'cleaned_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 6000\n","    })\n","    eval: Dataset({\n","        features: ['ID', 'text', 'cleaned_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1801\n","    })\n","})\n"]}],"source":["# Rename \"stars\" column to \"labels\" only in the train split\n","tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].rename_column(\"stars\", \"labels\")\n","\n","print(tokenized_datasets)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:29.495031Z","iopub.status.busy":"2024-03-16T15:38:29.494671Z","iopub.status.idle":"2024-03-16T15:38:29.510121Z","shell.execute_reply":"2024-03-16T15:38:29.509058Z","shell.execute_reply.started":"2024-03-16T15:38:29.494999Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets[\"train\"]['labels'][2]"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:29.511540Z","iopub.status.busy":"2024-03-16T15:38:29.511270Z","iopub.status.idle":"2024-03-16T15:38:43.549672Z","shell.execute_reply":"2024-03-16T15:38:43.548212Z","shell.execute_reply.started":"2024-03-16T15:38:29.511506Z"},"trusted":true},"outputs":[],"source":["!pip -q install evaluate"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:43.552031Z","iopub.status.busy":"2024-03-16T15:38:43.551609Z","iopub.status.idle":"2024-03-16T15:38:57.764898Z","shell.execute_reply":"2024-03-16T15:38:57.763646Z","shell.execute_reply.started":"2024-03-16T15:38:43.551982Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\n","Collecting accelerate\n","  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.27.2\n","    Uninstalling accelerate-0.27.2:\n","      Successfully uninstalled accelerate-0.27.2\n","Successfully installed accelerate-0.28.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install accelerate -U"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:38:57.769728Z","iopub.status.busy":"2024-03-16T15:38:57.769292Z","iopub.status.idle":"2024-03-16T15:39:13.372907Z","shell.execute_reply":"2024-03-16T15:39:13.371852Z","shell.execute_reply.started":"2024-03-16T15:38:57.769693Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-16 15:38:59.712372: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-16 15:38:59.712494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-16 15:38:59.855775: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea6e3fb67e2f4ab387431d31a43302a4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# On the Stability of Fine-Tuning BERT: Misconceptions, Explanations, and Strong Baselines https://arxiv.org/pdf/2006.04884.pdf \n","# reference paper for hyperparameter fine tuning for pretrained models\n","# I used RoBERTa with hyper parameters chosen from the research paper \n","# https://huggingface.co/docs/transformers/en/training\n","\n","from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n","from transformers import AutoModelForSequenceClassification, BertForSequenceClassification, BertModel\n","from transformers import get_linear_schedule_with_warmup, EarlyStoppingCallback\n","import evaluate\n","from torch.nn import CrossEntropyLoss\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","num_labels=3\n","\n","#model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n","\n","#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n","\n","# Training args\n","training_args = TrainingArguments(\n","    \"/kaggle/working/test-trainer\",\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n","    weight_decay=0.1,\n","    save_strategy=\"no\",\n","    report_to=\"none\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    #gradient_accumulation_steps = 16,\n","    per_device_eval_batch_size= 16,\n","    disable_tqdm = False,\n","    warmup_steps=0,\n","    logging_steps = 8,\n","    fp16 = True,\n","    logging_dir='/kaggle/working/logs',\n","    #dataloader_num_workers = 8,\n",")\n","\n","'''training_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/test-trainer\",\n","    per_device_train_batch_size=16,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n","    report_to=\"none\"\n",")'''\n","\n","# Get total number of training steps\n","total_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n","\n","# Metric for validation error\n","def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"mrpc\") # F1 and Accuracy\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","# Customize the training loop to compute the loss\n","def compute_loss(model, inputs):\n","    labels = inputs.pop(\"labels\")\n","    outputs = model(**inputs)\n","    logits = outputs.logits\n","    return loss_function(logits, labels)\n","\n","'''def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }'''\n","\n","# loss function\n","\n","loss_function = CrossEntropyLoss()\n","\n","\n","# Define trainer\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"eval\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n","    )\n","\n","# Initialize optimizer\n","optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay, eps = 1e-6, betas=(0.9, 0.98))\n","\n","warmup_ratio = 0.1\n","\n","# Calculate number of warmup steps\n","warmup_steps = int(total_steps * warmup_ratio)\n","\n","# Initialize scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=warmup_steps,\n","    num_training_steps=total_steps\n",")\n","\n","# Set scheduler to Trainer\n","trainer.scheduler = scheduler\n","\n","trainer.compute_loss = compute_loss"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T15:39:13.374905Z","iopub.status.busy":"2024-03-16T15:39:13.374406Z","iopub.status.idle":"2024-03-16T15:53:33.937561Z","shell.execute_reply":"2024-03-16T15:53:33.936604Z","shell.execute_reply.started":"2024-03-16T15:39:13.374870Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1125/1125 14:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.743700</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.588200</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.501900</td>\n","      <td>No log</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1125, training_loss=0.6277527058919271, metrics={'train_runtime': 860.2059, 'train_samples_per_second': 20.925, 'train_steps_per_second': 1.308, 'total_flos': 3780569809507680.0, 'train_loss': 0.6277527058919271, 'epoch': 3.0})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4543119,"sourceId":7766798,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
